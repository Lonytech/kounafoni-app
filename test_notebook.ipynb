{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:05:53.952401Z",
     "start_time": "2024-05-08T12:05:52.748618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import dateparser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from pathlib import Path"
   ],
   "id": "21445bbf06706b51",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:05:53.956683Z",
     "start_time": "2024-05-08T12:05:53.953570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_soup_parser(url):\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.text, \"html.parser\")"
   ],
   "id": "77a7450f57fa5c99",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:05:53.964106Z",
     "start_time": "2024-05-08T12:05:53.957625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_mali_jet_page_list_of_articles(num_page):\n",
    "    soup = get_soup_parser(url=f\"https://malijet.com/a_la_une_du_mali/?page={num_page}\")\n",
    "    articles = soup.find(\"div\", id=\"v_container\").find_all(\"div\", class_=\"card\")\n",
    "    titles, source_papers, dates, links = [], [], [], []\n",
    "    print('Getting list of articles...')\n",
    "    for article in tqdm(articles[:-1]):\n",
    "        header = article.find(\"div\", class_=\"card-header\")\n",
    "        link = header.find(\"a\", href=True)\n",
    "        title = None if not header else header.text.strip().split(\"\\n\")[-1]\n",
    "        infos = article.find(\"div\", class_=\"card-body\")\n",
    "        infos = None if not infos else infos.text.strip().split(\"\\n\")\n",
    "        \n",
    "        titles.append(title)\n",
    "        source_papers.append(None if not infos else infos[0])\n",
    "        dates.append(None if not infos or not dateparser.parse(infos[1]) else dateparser.parse(infos[1]).date())\n",
    "        links.append(link['href'])\n",
    "        # print(\"*\"*100)\n",
    "    return pd.DataFrame({\"title\": titles, \"source_paper\": source_papers, \"date\": dates, \"link\": links})"
   ],
   "id": "914146809b3a7a16",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extract info from one article\n",
   "id": "e575c235a38f5bac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:05:57.064428Z",
     "start_time": "2024-05-08T12:05:57.060364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_article_content(article_link):\n",
    "    soup = get_soup_parser(url=article_link)\n",
    "    \n",
    "    # get content\n",
    "    content = \" \".join(paragraph.text for paragraph in soup.find_all(\"div\", dir=\"auto\") if not paragraph.text.isspace())\n",
    "    \n",
    "    # TODO : We must implement a way to parse the article's author and return it as a tuple with \"content\"\n",
    "    # author = \"\"\n",
    "    \n",
    "    if content != '':\n",
    "        return content\n",
    "    else:\n",
    "        return unicodedata.normalize(\"NFKD\", \" \".join(soup.find(\"div\", class_=\"card-header\").text.split('Date : ')[1].split('\\n')[1:])).strip().replace(\"     \", \" \")"
   ],
   "id": "f6140d6fd0fcb4f1",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:05:57.068699Z",
     "start_time": "2024-05-08T12:05:57.065626Z"
    }
   },
   "cell_type": "code",
   "source": "new_article_link = \"https://malijet.com/a_la_une_du_mali/290531-industrie--le-president-assimi-goita-a-recu-lâ€™ancien-footballeur.html\"",
   "id": "ceea7e747900819",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:05:59.753702Z",
     "start_time": "2024-05-08T12:05:57.069791Z"
    }
   },
   "cell_type": "code",
   "source": "fetch_article_content(new_article_link)",
   "id": "8ffb36c42127e895",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Second part : Parsing using date",
   "id": "b20f56f71dfb35c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:05:59.766862Z",
     "start_time": "2024-05-08T12:05:59.754706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "begin_date = \"2024-04-27\"\n",
    "end_date = \"2024-05-08\" #today\n",
    "\n",
    "# parse them\n",
    "begin_date = dateparser.parse(begin_date).date()\n",
    "end_date = dateparser.parse(end_date).date()"
   ],
   "id": "80319b05fdb70481",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:06:06.176560Z",
     "start_time": "2024-05-08T12:05:59.767830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "page_number = 1\n",
    "articles_to_fetch_df = pd.DataFrame(columns=[\"title\", \"source_paper\", \"date\", \"link\"])\n",
    "contents = []\n",
    "current_date = end_date\n",
    "while begin_date <= current_date:\n",
    "    print(f\"fetching article from page {page_number} ...\")\n",
    "    articles_to_fetch_df = pd.concat([articles_to_fetch_df, get_mali_jet_page_list_of_articles(page_number)])\n",
    "    page_number+=1\n",
    "    current_date = articles_to_fetch_df.date.min()\n",
    "\n",
    "articles_to_fetch_df.query(\"date >= @begin_date and date <= @end_date\")"
   ],
   "id": "abefb3d88b9f1c0a",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:06:06.182202Z",
     "start_time": "2024-05-08T12:06:06.177693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CSV_DIR = Path().resolve() / 'data' / 'malijet' / 'source.csv'\n",
    "CSV_DIR"
   ],
   "id": "bd134ab43ed5fe92",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:06:06.202390Z",
     "start_time": "2024-05-08T12:06:06.183837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subset_fetching_articles_df = articles_to_fetch_df.query(\"date >= @begin_date and date <= @end_date\").copy()\n",
    "article_contents, new_titles = [], []\n",
    "existing_article_titles = pd.read_csv(CSV_DIR, sep='\\t').title.tolist()\n",
    "for _, row in tqdm(subset_fetching_articles_df.iterrows(), total=subset_fetching_articles_df.shape[0]):\n",
    "    if row.title not in existing_article_titles:\n",
    "        new_titles.append(row.title)\n",
    "        article_contents.append(fetch_article_content(row.link))\n",
    "if article_contents:\n",
    "    print(\"New articles found, writing article contents to file...\")\n",
    "    subset_fetching_articles_df.query(\"title in @new_titles\").assign(content=article_contents).to_csv(CSV_DIR, mode='a', sep='\\t', index=False)\n",
    "else:\n",
    "    print(\"No new articles found, skipping...\")"
   ],
   "id": "3fb398fa6d4877e3",
   "execution_count": 11,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
